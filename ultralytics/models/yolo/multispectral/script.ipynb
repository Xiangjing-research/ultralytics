{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb70d52c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.51  Python-3.8.17 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1060, 6144MiB)\n",
      "\u001B[34m\u001B[1mengine\\trainer: \u001B[0mtask=multispectral, mode=train, model=../../../cfg/models/v8/yolov8l-C2f_RepVit-CSFusion.yaml, data=../../../cfg/datasets/LLVIP.yaml, epochs=300, time=None, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=v8_multispectral, name=train_C2f_RepVit-CSFusion_LLVIP, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=0, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=v8_multispectral\\train_C2f_RepVit-CSFusion_LLVIP\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset '../../../cfg/datasets/LLVIP.yaml' error  \nDataset '../../../cfg/datasets/LLVIP.yaml' images not found , missing path 'E:\\DataSets\\LLVIP\\val-visible.txt'\nNote dataset download directory is 'D:\\PycharmProjects\\datasets'. You can update this in 'C:\\Users\\Administrator\\AppData\\Roaming\\Ultralytics\\settings.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32mE:\\PycharmProjects\\ultralytics\\ultralytics\\engine\\trainer.py:530\u001B[0m, in \u001B[0;36mBaseTrainer.get_dataset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    524\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myaml\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myml\u001B[39m\u001B[38;5;124m\"\u001B[39m} \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtask \u001B[38;5;129;01min\u001B[39;00m {\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdetect\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    526\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msegment\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    527\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpose\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    528\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    529\u001B[0m }:\n\u001B[1;32m--> 530\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_det_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myaml_file\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m data:\n",
      "File \u001B[1;32mE:\\PycharmProjects\\ultralytics\\ultralytics\\data\\utils.py:328\u001B[0m, in \u001B[0;36mcheck_det_dataset\u001B[1;34m(dataset, autodownload)\u001B[0m\n\u001B[0;32m    327\u001B[0m     m \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mNote dataset download directory is \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDATASETS_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. You can update this in \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mSETTINGS_YAML\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 328\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(m)\n\u001B[0;32m    329\u001B[0m t \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: \nDataset '../../../cfg/datasets/LLVIP.yaml' images not found ⚠️, missing path 'E:\\DataSets\\LLVIP\\val-visible.txt'\nNote dataset download directory is 'D:\\PycharmProjects\\datasets'. You can update this in 'C:\\Users\\Administrator\\AppData\\Roaming\\Ultralytics\\settings.yaml'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 6\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01multralytics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01myolo\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmultispectral\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultispectralDetectionTrainer\n\u001B[0;32m      4\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(task\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmultispectral\u001B[39m\u001B[38;5;124m'\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../../cfg/models/v8/yolov8l-C2f_RepVit-CSFusion.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      5\u001B[0m             data\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../../cfg/datasets/LLVIP.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m,epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m300\u001B[39m, batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,project\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv8_multispectral\u001B[39m\u001B[38;5;124m'\u001B[39m,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_C2f_RepVit-CSFusion_LLVIP\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mMultispectralDetectionTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43moverrides\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32mE:\\PycharmProjects\\ultralytics\\ultralytics\\engine\\trainer.py:132\u001B[0m, in \u001B[0;36mBaseTrainer.__init__\u001B[1;34m(self, cfg, overrides, _callbacks)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m check_model_file_from_stem(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mmodel)  \u001B[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch_distributed_zero_first(RANK):  \u001B[38;5;66;03m# avoid auto-downloading dataset multiple times\u001B[39;00m\n\u001B[1;32m--> 132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainset, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtestset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mema \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;66;03m# Optimization utils init\u001B[39;00m\n",
      "File \u001B[1;32mE:\\PycharmProjects\\ultralytics\\ultralytics\\engine\\trainer.py:534\u001B[0m, in \u001B[0;36mBaseTrainer.get_dataset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    532\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myaml_file\u001B[39m\u001B[38;5;124m\"\u001B[39m]  \u001B[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001B[39;00m\n\u001B[0;32m    533\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 534\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(emojis(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclean_url(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdata)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m error ❌ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m], data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Dataset '../../../cfg/datasets/LLVIP.yaml' error  \nDataset '../../../cfg/datasets/LLVIP.yaml' images not found , missing path 'E:\\DataSets\\LLVIP\\val-visible.txt'\nNote dataset download directory is 'D:\\PycharmProjects\\datasets'. You can update this in 'C:\\Users\\Administrator\\AppData\\Roaming\\Ultralytics\\settings.yaml'"
     ]
    }
   ],
   "source": [
    "from os import system\n",
    "from ultralytics.models.yolo.multispectral import MultispectralDetectionTrainer\n",
    "\n",
    "args = dict(task='multispectral', mode='train', model='../../../cfg/models/v8/yolov8l-C2f_RepVit-CSFusion.yaml',\n",
    "            data='../../../cfg/datasets/LLVIP.yaml',epochs=300, batch=4,project='v8_multispectral',name='train_C2f_RepVit-CSFusion_LLVIP')\n",
    "trainer = MultispectralDetectionTrainer(overrides=args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320e956f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.201  Python-3.8.13 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "YOLOv8l-C2f_FasterNet-DFMDA-2 summary: 731 layers, 40751955 parameters, 0 gradients\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning D:\\LLVIP\\labels\\visible\\test.cache... 3463 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3463/3463 [\u001B[0m\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning D:\\LLVIP\\labels\\infrared\\test.cache... 3463 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3463/3463 \u001B[0m\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv8_multispectral/train-C2f_FasterNet-DFMDA-LLVIP4/weights/best.pt\u001B[39m\u001B[38;5;124m'\u001B[39m,data\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../../cfg/datasets/LLVIP.yaml\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m validator \u001B[38;5;241m=\u001B[39m MultispectralDetectionValidator(args\u001B[38;5;241m=\u001B[39margs)\n\u001B[1;32m----> 5\u001B[0m validator()\n",
      "File \u001B[1;32mD:\\ProgramData\\Miniconda3\\envs\\TransTrack\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[1;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\pycharmproject\\ultralytics\\ultralytics\\models\\yolo\\multispectral\\val.py:260\u001B[0m, in \u001B[0;36mMultispectralDetectionValidator.__call__\u001B[1;34m(self, trainer, model)\u001B[0m\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_dataloader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msplit)[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    257\u001B[0m                                                              \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msplit)[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mbatch)\n\u001B[0;32m    259\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m--> 260\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarmup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgsz\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimgsz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimgsz\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# warmup\u001B[39;00m\n\u001B[0;32m    262\u001B[0m dt \u001B[38;5;241m=\u001B[39m Profile(), Profile(), Profile(), Profile()\n\u001B[0;32m    263\u001B[0m bar \u001B[38;5;241m=\u001B[39m TQDM(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_desc(), total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataloader))\n",
      "File \u001B[1;32md:\\pycharmproject\\ultralytics\\ultralytics\\nn\\autobackend.py:480\u001B[0m, in \u001B[0;36mAutoBackend.warmup\u001B[1;34m(self, imgsz)\u001B[0m\n\u001B[0;32m    478\u001B[0m im \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(\u001B[38;5;241m*\u001B[39mimgsz, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mhalf \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mfloat, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# input\u001B[39;00m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjit \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m):  \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m--> 480\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\pycharmproject\\ultralytics\\ultralytics\\nn\\autobackend.py:347\u001B[0m, in \u001B[0;36mAutoBackend.forward\u001B[1;34m(self, im, augment, visualize)\u001B[0m\n\u001B[0;32m    344\u001B[0m     im \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpt \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnn_module:  \u001B[38;5;66;03m# PyTorch\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(im, augment\u001B[38;5;241m=\u001B[39maugment, visualize\u001B[38;5;241m=\u001B[39mvisualize) \u001B[38;5;28;01mif\u001B[39;00m augment \u001B[38;5;129;01mor\u001B[39;00m visualize \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjit:  \u001B[38;5;66;03m# TorchScript\u001B[39;00m\n\u001B[0;32m    349\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(im)\n",
      "File \u001B[1;32mD:\\ProgramData\\Miniconda3\\envs\\TransTrack\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\pycharmproject\\ultralytics\\ultralytics\\nn\\tasks.py:42\u001B[0m, in \u001B[0;36mBaseModel.forward\u001B[1;34m(self, x, *args, **kwargs)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):  \u001B[38;5;66;03m# for cases of training and validating while training.\u001B[39;00m\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\pycharmproject\\ultralytics\\ultralytics\\nn\\tasks.py:59\u001B[0m, in \u001B[0;36mBaseModel.predict\u001B[1;34m(self, x, profile, visualize, augment)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m augment:\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_predict_augment(x)\n\u001B[1;32m---> 59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\pycharmproject\\ultralytics\\ultralytics\\nn\\tasks.py:900\u001B[0m, in \u001B[0;36mMultispectralDetectionModel._predict_once\u001B[1;34m(self, x, profile, visualize)\u001B[0m\n\u001B[0;32m    885\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    886\u001B[0m \u001B[38;5;124;03mPerform a forward pass through the network.\u001B[39;00m\n\u001B[0;32m    887\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    895\u001B[0m \u001B[38;5;124;03m    (torch.Tensor): The last output of the model.\u001B[39;00m\n\u001B[0;32m    896\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    897\u001B[0m \u001B[38;5;66;03m# profile = True\u001B[39;00m\n\u001B[0;32m    898\u001B[0m \u001B[38;5;66;03m#将输入的img分成rgb和ir\u001B[39;00m\n\u001B[0;32m    899\u001B[0m \u001B[38;5;66;03m# x = torch.split(x, 3, dim=1)\u001B[39;00m\n\u001B[1;32m--> 900\u001B[0m x, ir \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;241m0\u001B[39m], \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m    902\u001B[0m y, dt \u001B[38;5;241m=\u001B[39m [], []  \u001B[38;5;66;03m# outputs\u001B[39;00m\n\u001B[0;32m    903\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel:\n",
      "\u001B[1;31mIndexError\u001B[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "from ultralytics.models.yolo.multispectral import MultispectralDetectionValidator\n",
    "\n",
    "args = dict(mode='val', model='v8_multispectral/train-C2f_FasterNet-DFMDA-LLVIP4/weights/best.pt',data='../../../cfg/datasets/LLVIP.yaml')\n",
    "validator = MultispectralDetectionValidator(args=args)\n",
    "validator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b33cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer                                     name  gradient   parameters                shape         mu      sigma\n",
      "    0                      model.0.conv.weight      True         1728        [64, 3, 3, 3]   0.000641       0.11 torch.float32\n",
      "    1                        model.0.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "    2                          model.0.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "    3                      model.1.conv.weight      True        73728      [128, 64, 3, 3]   -0.00013     0.0241 torch.float32\n",
      "    4                        model.1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "    5                          model.1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "    6                  model.2.cv1.conv.weight      True        16384     [128, 128, 1, 1]  -0.000368     0.0513 torch.float32\n",
      "    7                    model.2.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "    8                      model.2.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "    9                  model.2.cv2.conv.weight      True        40960     [128, 320, 1, 1]   3.69e-05     0.0323 torch.float32\n",
      "   10                    model.2.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   11                      model.2.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   12              model.2.m.0.cv1.conv.weight      True        36864       [64, 64, 3, 3]  -6.43e-05     0.0241 torch.float32\n",
      "   13                model.2.m.0.cv1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "   14                  model.2.m.0.cv1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "   15              model.2.m.0.cv2.conv.weight      True        36864       [64, 64, 3, 3]   0.000135     0.0241 torch.float32\n",
      "   16                model.2.m.0.cv2.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "   17                  model.2.m.0.cv2.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "   18              model.2.m.1.cv1.conv.weight      True        36864       [64, 64, 3, 3]   4.93e-05     0.0241 torch.float32\n",
      "   19                model.2.m.1.cv1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "   20                  model.2.m.1.cv1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "   21              model.2.m.1.cv2.conv.weight      True        36864       [64, 64, 3, 3]  -2.18e-05      0.024 torch.float32\n",
      "   22                model.2.m.1.cv2.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "   23                  model.2.m.1.cv2.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "   24              model.2.m.2.cv1.conv.weight      True        36864       [64, 64, 3, 3]   6.26e-05      0.024 torch.float32\n",
      "   25                model.2.m.2.cv1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "   26                  model.2.m.2.cv1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "   27              model.2.m.2.cv2.conv.weight      True        36864       [64, 64, 3, 3]   8.18e-05     0.0241 torch.float32\n",
      "   28                model.2.m.2.cv2.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "   29                  model.2.m.2.cv2.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "   30                      model.3.conv.weight      True       294912     [256, 128, 3, 3]  -5.95e-05      0.017 torch.float32\n",
      "   31                        model.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "   32                          model.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "   33                  model.4.cv1.conv.weight      True        65536     [256, 256, 1, 1]  -0.000347     0.0361 torch.float32\n",
      "   34                    model.4.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "   35                      model.4.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "   36                  model.4.cv2.conv.weight      True       262144    [256, 1024, 1, 1]   5.11e-05     0.0181 torch.float32\n",
      "   37                    model.4.cv2.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "   38                      model.4.cv2.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "   39              model.4.m.0.cv1.conv.weight      True       147456     [128, 128, 3, 3]   5.96e-06      0.017 torch.float32\n",
      "   40                model.4.m.0.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   41                  model.4.m.0.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   42              model.4.m.0.cv2.conv.weight      True       147456     [128, 128, 3, 3]   7.31e-05      0.017 torch.float32\n",
      "   43                model.4.m.0.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   44                  model.4.m.0.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   45              model.4.m.1.cv1.conv.weight      True       147456     [128, 128, 3, 3]   1.76e-05      0.017 torch.float32\n",
      "   46                model.4.m.1.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   47                  model.4.m.1.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   48              model.4.m.1.cv2.conv.weight      True       147456     [128, 128, 3, 3]   4.13e-06      0.017 torch.float32\n",
      "   49                model.4.m.1.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   50                  model.4.m.1.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   51              model.4.m.2.cv1.conv.weight      True       147456     [128, 128, 3, 3]   6.88e-05     0.0171 torch.float32\n",
      "   52                model.4.m.2.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   53                  model.4.m.2.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   54              model.4.m.2.cv2.conv.weight      True       147456     [128, 128, 3, 3]   1.13e-05      0.017 torch.float32\n",
      "   55                model.4.m.2.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   56                  model.4.m.2.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   57              model.4.m.3.cv1.conv.weight      True       147456     [128, 128, 3, 3]  -5.94e-05      0.017 torch.float32\n",
      "   58                model.4.m.3.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   59                  model.4.m.3.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   60              model.4.m.3.cv2.conv.weight      True       147456     [128, 128, 3, 3]   4.18e-05      0.017 torch.float32\n",
      "   61                model.4.m.3.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   62                  model.4.m.3.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   63              model.4.m.4.cv1.conv.weight      True       147456     [128, 128, 3, 3]  -1.41e-05      0.017 torch.float32\n",
      "   64                model.4.m.4.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   65                  model.4.m.4.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   66              model.4.m.4.cv2.conv.weight      True       147456     [128, 128, 3, 3]   6.67e-05      0.017 torch.float32\n",
      "   67                model.4.m.4.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   68                  model.4.m.4.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   69              model.4.m.5.cv1.conv.weight      True       147456     [128, 128, 3, 3]  -6.22e-06      0.017 torch.float32\n",
      "   70                model.4.m.5.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   71                  model.4.m.5.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   72              model.4.m.5.cv2.conv.weight      True       147456     [128, 128, 3, 3]  -6.39e-05      0.017 torch.float32\n",
      "   73                model.4.m.5.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "   74                  model.4.m.5.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "   75                  model.5.cv1.conv.weight      True       131072     [512, 256, 1, 1]  -1.28e-06     0.0362 torch.float32\n",
      "   76                    model.5.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "   77                      model.5.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "   78                  model.5.cv2.conv.weight      True         4608       [512, 1, 3, 3]    0.00409      0.193 torch.float32\n",
      "   79                    model.5.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "   80                      model.5.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "   81                  model.6.cv1.conv.weight      True       262144     [512, 512, 1, 1]  -4.59e-05     0.0255 torch.float32\n",
      "   82                    model.6.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "   83                      model.6.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "   84                  model.6.cv2.conv.weight      True  1.04858e+06    [512, 2048, 1, 1]  -6.51e-06     0.0128 torch.float32\n",
      "   85                    model.6.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "   86                      model.6.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "   87              model.6.m.0.cv1.conv.weight      True       589824     [256, 256, 3, 3]   8.59e-06      0.012 torch.float32\n",
      "   88                model.6.m.0.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "   89                  model.6.m.0.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "   90              model.6.m.0.cv2.conv.weight      True       589824     [256, 256, 3, 3]   1.84e-05      0.012 torch.float32\n",
      "   91                model.6.m.0.cv2.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "   92                  model.6.m.0.cv2.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "   93              model.6.m.1.cv1.conv.weight      True       589824     [256, 256, 3, 3]  -2.26e-05      0.012 torch.float32\n",
      "   94                model.6.m.1.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "   95                  model.6.m.1.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "   96              model.6.m.1.cv2.conv.weight      True       589824     [256, 256, 3, 3]   4.09e-06      0.012 torch.float32\n",
      "   97                model.6.m.1.cv2.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "   98                  model.6.m.1.cv2.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "   99              model.6.m.2.cv1.conv.weight      True       589824     [256, 256, 3, 3]  -1.58e-05      0.012 torch.float32\n",
      "  100                model.6.m.2.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  101                  model.6.m.2.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  102              model.6.m.2.cv2.conv.weight      True       589824     [256, 256, 3, 3]   1.28e-05      0.012 torch.float32\n",
      "  103                model.6.m.2.cv2.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  104                  model.6.m.2.cv2.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  105              model.6.m.3.cv1.conv.weight      True       589824     [256, 256, 3, 3]   2.38e-05      0.012 torch.float32\n",
      "  106                model.6.m.3.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  107                  model.6.m.3.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  108              model.6.m.3.cv2.conv.weight      True       589824     [256, 256, 3, 3]    4.1e-06      0.012 torch.float32\n",
      "  109                model.6.m.3.cv2.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  110                  model.6.m.3.cv2.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  111              model.6.m.4.cv1.conv.weight      True       589824     [256, 256, 3, 3]  -8.97e-06      0.012 torch.float32\n",
      "  112                model.6.m.4.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  113                  model.6.m.4.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  114              model.6.m.4.cv2.conv.weight      True       589824     [256, 256, 3, 3]  -6.71e-06      0.012 torch.float32\n",
      "  115                model.6.m.4.cv2.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  116                  model.6.m.4.cv2.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  117              model.6.m.5.cv1.conv.weight      True       589824     [256, 256, 3, 3]  -2.74e-06      0.012 torch.float32\n",
      "  118                model.6.m.5.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  119                  model.6.m.5.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  120              model.6.m.5.cv2.conv.weight      True       589824     [256, 256, 3, 3]    1.2e-05      0.012 torch.float32\n",
      "  121                model.6.m.5.cv2.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  122                  model.6.m.5.cv2.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  123                  model.7.cv1.conv.weight      True       262144     [512, 512, 1, 1]  -3.03e-05     0.0255 torch.float32\n",
      "  124                    model.7.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  125                      model.7.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  126                  model.7.cv2.conv.weight      True         4608       [512, 1, 3, 3]   -0.00102      0.191 torch.float32\n",
      "  127                    model.7.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  128                      model.7.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  129                  model.8.cv1.conv.weight      True       262144     [512, 512, 1, 1]  -3.25e-05     0.0255 torch.float32\n",
      "  130                    model.8.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  131                      model.8.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  132                  model.8.cv2.conv.weight      True       655360    [512, 1280, 1, 1]  -7.96e-06     0.0161 torch.float32\n",
      "  133                    model.8.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  134                      model.8.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  135            model.8.m.0.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00483      0.194 torch.float32\n",
      "  136              model.8.m.0.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  137                model.8.m.0.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  138            model.8.m.0.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]   4.51e-05     0.0362 torch.float32\n",
      "  139              model.8.m.0.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  140                model.8.m.0.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  141            model.8.m.0.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]    -0.0045      0.192 torch.float32\n",
      "  142              model.8.m.0.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  143                model.8.m.0.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  144            model.8.m.0.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   5.98e-05     0.0255 torch.float32\n",
      "  145              model.8.m.0.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  146                model.8.m.0.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  147            model.8.m.0.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]   -0.00503      0.191 torch.float32\n",
      "  148              model.8.m.0.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  149                model.8.m.0.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  150            model.8.m.1.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00155      0.191 torch.float32\n",
      "  151              model.8.m.1.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  152                model.8.m.1.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  153            model.8.m.1.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]   0.000108     0.0361 torch.float32\n",
      "  154              model.8.m.1.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  155                model.8.m.1.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  156            model.8.m.1.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]   -0.00383      0.192 torch.float32\n",
      "  157              model.8.m.1.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  158                model.8.m.1.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  159            model.8.m.1.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   0.000105     0.0255 torch.float32\n",
      "  160              model.8.m.1.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  161                model.8.m.1.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  162            model.8.m.1.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]  -0.000917      0.192 torch.float32\n",
      "  163              model.8.m.1.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  164                model.8.m.1.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  165            model.8.m.2.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00208      0.194 torch.float32\n",
      "  166              model.8.m.2.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  167                model.8.m.2.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  168            model.8.m.2.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]   0.000207      0.036 torch.float32\n",
      "  169              model.8.m.2.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  170                model.8.m.2.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  171            model.8.m.2.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]  -0.000305      0.192 torch.float32\n",
      "  172              model.8.m.2.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  173                model.8.m.2.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  174            model.8.m.2.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   4.22e-05     0.0255 torch.float32\n",
      "  175              model.8.m.2.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  176                model.8.m.2.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  177            model.8.m.2.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]    0.00425      0.193 torch.float32\n",
      "  178              model.8.m.2.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  179                model.8.m.2.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  180                  model.9.cv1.conv.weight      True       131072     [256, 512, 1, 1]   -5.1e-06     0.0255 torch.float32\n",
      "  181                    model.9.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  182                      model.9.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  183                  model.9.cv2.conv.weight      True       524288    [512, 1024, 1, 1]  -4.22e-05      0.018 torch.float32\n",
      "  184                    model.9.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  185                      model.9.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  186                 model.10.cv1.conv.weight      True       262144     [512, 512, 1, 1]   7.76e-05     0.0256 torch.float32\n",
      "  187                   model.10.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  188                     model.10.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  189                 model.10.cv2.conv.weight      True       262144     [512, 512, 1, 1]  -3.52e-05     0.0255 torch.float32\n",
      "  190                   model.10.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  191                     model.10.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  192            model.10.attn.qkv.conv.weight      True       131072     [512, 256, 1, 1]   8.41e-05      0.036 torch.float32\n",
      "  193              model.10.attn.qkv.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  194                model.10.attn.qkv.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  195           model.10.attn.proj.conv.weight      True        65536     [256, 256, 1, 1]    3.6e-05     0.0361 torch.float32\n",
      "  196             model.10.attn.proj.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  197               model.10.attn.proj.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  198             model.10.attn.pe.conv.weight      True         2304       [256, 1, 3, 3]  -0.000672      0.191 torch.float32\n",
      "  199               model.10.attn.pe.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  200                 model.10.attn.pe.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  201               model.10.ffn.0.conv.weight      True       131072     [512, 256, 1, 1]   3.37e-05     0.0362 torch.float32\n",
      "  202                 model.10.ffn.0.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  203                   model.10.ffn.0.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  204               model.10.ffn.1.conv.weight      True       131072     [256, 512, 1, 1]   8.64e-05     0.0255 torch.float32\n",
      "  205                 model.10.ffn.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  206                   model.10.ffn.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  207                 model.13.cv1.conv.weight      True       524288    [512, 1024, 1, 1]  -1.53e-05      0.018 torch.float32\n",
      "  208                   model.13.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  209                     model.13.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  210                 model.13.cv2.conv.weight      True       655360    [512, 1280, 1, 1]   2.46e-05     0.0161 torch.float32\n",
      "  211                   model.13.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  212                     model.13.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  213           model.13.m.0.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]   -0.00284      0.192 torch.float32\n",
      "  214             model.13.m.0.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  215               model.13.m.0.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  216           model.13.m.0.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]   0.000229      0.036 torch.float32\n",
      "  217             model.13.m.0.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  218               model.13.m.0.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  219           model.13.m.0.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]   -0.00177      0.193 torch.float32\n",
      "  220             model.13.m.0.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  221               model.13.m.0.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  222           model.13.m.0.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]  -1.49e-05     0.0255 torch.float32\n",
      "  223             model.13.m.0.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  224               model.13.m.0.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  225           model.13.m.0.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]    0.00441      0.192 torch.float32\n",
      "  226             model.13.m.0.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  227               model.13.m.0.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  228           model.13.m.1.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00171      0.192 torch.float32\n",
      "  229             model.13.m.1.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  230               model.13.m.1.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  231           model.13.m.1.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]  -0.000146     0.0361 torch.float32\n",
      "  232             model.13.m.1.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  233               model.13.m.1.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  234           model.13.m.1.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]     -0.001      0.193 torch.float32\n",
      "  235             model.13.m.1.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  236               model.13.m.1.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  237           model.13.m.1.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   -4.8e-05     0.0255 torch.float32\n",
      "  238             model.13.m.1.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  239               model.13.m.1.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  240           model.13.m.1.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]  -1.54e-05      0.193 torch.float32\n",
      "  241             model.13.m.1.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  242               model.13.m.1.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  243           model.13.m.2.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]   -0.00307      0.192 torch.float32\n",
      "  244             model.13.m.2.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  245               model.13.m.2.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  246           model.13.m.2.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]   2.04e-06     0.0361 torch.float32\n",
      "  247             model.13.m.2.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  248               model.13.m.2.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  249           model.13.m.2.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]   -0.00108      0.193 torch.float32\n",
      "  250             model.13.m.2.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  251               model.13.m.2.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  252           model.13.m.2.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   2.02e-05     0.0255 torch.float32\n",
      "  253             model.13.m.2.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  254               model.13.m.2.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  255           model.13.m.2.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]   -0.00465      0.191 torch.float32\n",
      "  256             model.13.m.2.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  257               model.13.m.2.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  258                 model.16.cv1.conv.weight      True       196608     [256, 768, 1, 1]  -3.96e-05     0.0208 torch.float32\n",
      "  259                   model.16.cv1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  260                     model.16.cv1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  261                 model.16.cv2.conv.weight      True       163840     [256, 640, 1, 1]  -4.34e-05     0.0228 torch.float32\n",
      "  262                   model.16.cv2.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  263                     model.16.cv2.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  264             model.16.m.0.cv1.conv.weight      True       147456     [128, 128, 3, 3]   2.82e-05      0.017 torch.float32\n",
      "  265               model.16.m.0.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "  266                 model.16.m.0.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "  267             model.16.m.0.cv2.conv.weight      True       147456     [128, 128, 3, 3]  -2.73e-05      0.017 torch.float32\n",
      "  268               model.16.m.0.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "  269                 model.16.m.0.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "  270             model.16.m.1.cv1.conv.weight      True       147456     [128, 128, 3, 3]   -5.4e-05      0.017 torch.float32\n",
      "  271               model.16.m.1.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "  272                 model.16.m.1.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "  273             model.16.m.1.cv2.conv.weight      True       147456     [128, 128, 3, 3]   -2.2e-06      0.017 torch.float32\n",
      "  274               model.16.m.1.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "  275                 model.16.m.1.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "  276             model.16.m.2.cv1.conv.weight      True       147456     [128, 128, 3, 3]  -5.47e-05      0.017 torch.float32\n",
      "  277               model.16.m.2.cv1.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "  278                 model.16.m.2.cv1.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "  279             model.16.m.2.cv2.conv.weight      True       147456     [128, 128, 3, 3]   6.16e-06      0.017 torch.float32\n",
      "  280               model.16.m.2.cv2.bn.weight      True          128                [128]          1          0 torch.float32\n",
      "  281                 model.16.m.2.cv2.bn.bias      True          128                [128]          0          0 torch.float32\n",
      "  282                     model.17.conv.weight      True       589824     [256, 256, 3, 3]   4.16e-06      0.012 torch.float32\n",
      "  283                       model.17.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  284                         model.17.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  285                 model.19.cv1.conv.weight      True       393216     [512, 768, 1, 1]   5.81e-05     0.0209 torch.float32\n",
      "  286                   model.19.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  287                     model.19.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  288                 model.19.cv2.conv.weight      True       655360    [512, 1280, 1, 1]    1.8e-05     0.0161 torch.float32\n",
      "  289                   model.19.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  290                     model.19.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  291           model.19.m.0.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]     0.0011      0.192 torch.float32\n",
      "  292             model.19.m.0.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  293               model.19.m.0.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  294           model.19.m.0.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]   2.49e-05     0.0362 torch.float32\n",
      "  295             model.19.m.0.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  296               model.19.m.0.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  297           model.19.m.0.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]   -0.00359      0.194 torch.float32\n",
      "  298             model.19.m.0.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  299               model.19.m.0.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  300           model.19.m.0.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   9.57e-05     0.0255 torch.float32\n",
      "  301             model.19.m.0.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  302               model.19.m.0.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  303           model.19.m.0.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]    0.00198      0.196 torch.float32\n",
      "  304             model.19.m.0.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  305               model.19.m.0.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  306           model.19.m.1.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]   0.000335       0.19 torch.float32\n",
      "  307             model.19.m.1.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  308               model.19.m.1.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  309           model.19.m.1.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]   9.41e-05     0.0361 torch.float32\n",
      "  310             model.19.m.1.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  311               model.19.m.1.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  312           model.19.m.1.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]    0.00502      0.194 torch.float32\n",
      "  313             model.19.m.1.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  314               model.19.m.1.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  315           model.19.m.1.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   7.31e-05     0.0255 torch.float32\n",
      "  316             model.19.m.1.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  317               model.19.m.1.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  318           model.19.m.1.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]    0.00279      0.189 torch.float32\n",
      "  319             model.19.m.1.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  320               model.19.m.1.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  321           model.19.m.2.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]   -0.00792      0.193 torch.float32\n",
      "  322             model.19.m.2.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  323               model.19.m.2.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  324           model.19.m.2.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]  -6.98e-05     0.0361 torch.float32\n",
      "  325             model.19.m.2.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  326               model.19.m.2.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  327           model.19.m.2.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]   -0.00312      0.193 torch.float32\n",
      "  328             model.19.m.2.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  329               model.19.m.2.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  330           model.19.m.2.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]  -0.000101     0.0255 torch.float32\n",
      "  331             model.19.m.2.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  332               model.19.m.2.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  333           model.19.m.2.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]    0.00652       0.19 torch.float32\n",
      "  334             model.19.m.2.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  335               model.19.m.2.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  336                 model.20.cv1.conv.weight      True       262144     [512, 512, 1, 1]   2.63e-06     0.0255 torch.float32\n",
      "  337                   model.20.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  338                     model.20.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  339                 model.20.cv2.conv.weight      True         4608       [512, 1, 3, 3]  -0.000209      0.192 torch.float32\n",
      "  340                   model.20.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  341                     model.20.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  342                 model.22.cv1.conv.weight      True       524288    [512, 1024, 1, 1]  -2.64e-05     0.0181 torch.float32\n",
      "  343                   model.22.cv1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  344                     model.22.cv1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  345                 model.22.cv2.conv.weight      True       655360    [512, 1280, 1, 1]   3.63e-05     0.0161 torch.float32\n",
      "  346                   model.22.cv2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  347                     model.22.cv2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  348           model.22.m.0.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00531      0.191 torch.float32\n",
      "  349             model.22.m.0.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  350               model.22.m.0.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  351           model.22.m.0.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]  -5.25e-05     0.0361 torch.float32\n",
      "  352             model.22.m.0.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  353               model.22.m.0.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  354           model.22.m.0.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]  -0.000748      0.193 torch.float32\n",
      "  355             model.22.m.0.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  356               model.22.m.0.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  357           model.22.m.0.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]    2.5e-05     0.0255 torch.float32\n",
      "  358             model.22.m.0.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  359               model.22.m.0.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  360           model.22.m.0.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]    0.00598      0.192 torch.float32\n",
      "  361             model.22.m.0.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  362               model.22.m.0.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  363           model.22.m.1.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00108      0.191 torch.float32\n",
      "  364             model.22.m.1.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  365               model.22.m.1.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  366           model.22.m.1.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]  -9.38e-06      0.036 torch.float32\n",
      "  367             model.22.m.1.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  368               model.22.m.1.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  369           model.22.m.1.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]    0.00203      0.193 torch.float32\n",
      "  370             model.22.m.1.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  371               model.22.m.1.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  372           model.22.m.1.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   6.49e-05     0.0255 torch.float32\n",
      "  373             model.22.m.1.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  374               model.22.m.1.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  375           model.22.m.1.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]   0.000508      0.192 torch.float32\n",
      "  376             model.22.m.1.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  377               model.22.m.1.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  378           model.22.m.2.cv1.0.conv.weight      True         2304       [256, 1, 3, 3]   -0.00193      0.193 torch.float32\n",
      "  379             model.22.m.2.cv1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  380               model.22.m.2.cv1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  381           model.22.m.2.cv1.1.conv.weight      True       131072     [512, 256, 1, 1]   0.000121     0.0361 torch.float32\n",
      "  382             model.22.m.2.cv1.1.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  383               model.22.m.2.cv1.1.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  384           model.22.m.2.cv1.2.conv.weight      True         4608       [512, 1, 3, 3]  -0.000384      0.194 torch.float32\n",
      "  385             model.22.m.2.cv1.2.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  386               model.22.m.2.cv1.2.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  387           model.22.m.2.cv1.3.conv.weight      True       131072     [256, 512, 1, 1]   2.44e-05     0.0255 torch.float32\n",
      "  388             model.22.m.2.cv1.3.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  389               model.22.m.2.cv1.3.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  390           model.22.m.2.cv1.4.conv.weight      True         2304       [256, 1, 3, 3]  -6.15e-05      0.196 torch.float32\n",
      "  391             model.22.m.2.cv1.4.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  392               model.22.m.2.cv1.4.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  393             model.23.cv2.0.0.conv.weight      True       147456      [64, 256, 3, 3]  -3.91e-05      0.012 torch.float32\n",
      "  394               model.23.cv2.0.0.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  395                 model.23.cv2.0.0.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  396             model.23.cv2.0.1.conv.weight      True        36864       [64, 64, 3, 3]   2.51e-05      0.024 torch.float32\n",
      "  397               model.23.cv2.0.1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  398                 model.23.cv2.0.1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  399                  model.23.cv2.0.2.weight      True         4096       [64, 64, 1, 1]   7.49e-05     0.0727 torch.float32\n",
      "  400                    model.23.cv2.0.2.bias      True           64                 [64]          1          0 torch.float32\n",
      "  401             model.23.cv2.1.0.conv.weight      True       294912      [64, 512, 3, 3]  -3.35e-06    0.00851 torch.float32\n",
      "  402               model.23.cv2.1.0.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  403                 model.23.cv2.1.0.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  404             model.23.cv2.1.1.conv.weight      True        36864       [64, 64, 3, 3]   0.000114     0.0241 torch.float32\n",
      "  405               model.23.cv2.1.1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  406                 model.23.cv2.1.1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  407                  model.23.cv2.1.2.weight      True         4096       [64, 64, 1, 1]   -0.00119     0.0725 torch.float32\n",
      "  408                    model.23.cv2.1.2.bias      True           64                 [64]          1          0 torch.float32\n",
      "  409             model.23.cv2.2.0.conv.weight      True       294912      [64, 512, 3, 3]   2.21e-05    0.00851 torch.float32\n",
      "  410               model.23.cv2.2.0.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  411                 model.23.cv2.2.0.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  412             model.23.cv2.2.1.conv.weight      True        36864       [64, 64, 3, 3]   6.06e-05      0.024 torch.float32\n",
      "  413               model.23.cv2.2.1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  414                 model.23.cv2.2.1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  415                  model.23.cv2.2.2.weight      True         4096       [64, 64, 1, 1]   0.000523      0.073 torch.float32\n",
      "  416                    model.23.cv2.2.2.bias      True           64                 [64]          1          0 torch.float32\n",
      "  417           model.23.cv3.0.0.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00337      0.194 torch.float32\n",
      "  418             model.23.cv3.0.0.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  419               model.23.cv3.0.0.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  420           model.23.cv3.0.0.1.conv.weight      True        65536     [256, 256, 1, 1]  -8.95e-05     0.0361 torch.float32\n",
      "  421             model.23.cv3.0.0.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  422               model.23.cv3.0.0.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  423           model.23.cv3.0.1.0.conv.weight      True         2304       [256, 1, 3, 3]  -0.000615       0.19 torch.float32\n",
      "  424             model.23.cv3.0.1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  425               model.23.cv3.0.1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  426           model.23.cv3.0.1.1.conv.weight      True        65536     [256, 256, 1, 1]   7.58e-05     0.0361 torch.float32\n",
      "  427             model.23.cv3.0.1.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  428               model.23.cv3.0.1.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  429                  model.23.cv3.0.2.weight      True        20480      [80, 256, 1, 1]    -0.0003      0.036 torch.float32\n",
      "  430                    model.23.cv3.0.2.bias      True           80                 [80]      -11.5   1.92e-06 torch.float32\n",
      "  431           model.23.cv3.1.0.0.conv.weight      True         4608       [512, 1, 3, 3]    0.00116      0.191 torch.float32\n",
      "  432             model.23.cv3.1.0.0.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  433               model.23.cv3.1.0.0.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  434           model.23.cv3.1.0.1.conv.weight      True       131072     [256, 512, 1, 1]   0.000104     0.0256 torch.float32\n",
      "  435             model.23.cv3.1.0.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  436               model.23.cv3.1.0.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  437           model.23.cv3.1.1.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00638      0.192 torch.float32\n",
      "  438             model.23.cv3.1.1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  439               model.23.cv3.1.1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  440           model.23.cv3.1.1.1.conv.weight      True        65536     [256, 256, 1, 1]   8.33e-06      0.036 torch.float32\n",
      "  441             model.23.cv3.1.1.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  442               model.23.cv3.1.1.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  443                  model.23.cv3.1.2.weight      True        20480      [80, 256, 1, 1]    0.00018      0.036 torch.float32\n",
      "  444                    model.23.cv3.1.2.bias      True           80                 [80]      -10.2          0 torch.float32\n",
      "  445           model.23.cv3.2.0.0.conv.weight      True         4608       [512, 1, 3, 3]   -0.00115      0.193 torch.float32\n",
      "  446             model.23.cv3.2.0.0.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  447               model.23.cv3.2.0.0.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  448           model.23.cv3.2.0.1.conv.weight      True       131072     [256, 512, 1, 1]   0.000156     0.0256 torch.float32\n",
      "  449             model.23.cv3.2.0.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  450               model.23.cv3.2.0.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  451           model.23.cv3.2.1.0.conv.weight      True         2304       [256, 1, 3, 3]   -0.00224      0.192 torch.float32\n",
      "  452             model.23.cv3.2.1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  453               model.23.cv3.2.1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  454           model.23.cv3.2.1.1.conv.weight      True        65536     [256, 256, 1, 1]   8.46e-05      0.036 torch.float32\n",
      "  455             model.23.cv3.2.1.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  456               model.23.cv3.2.1.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  457                  model.23.cv3.2.2.weight      True        20480      [80, 256, 1, 1]   0.000216     0.0361 torch.float32\n",
      "  458                    model.23.cv3.2.2.bias      True           80                 [80]      -8.76          0 torch.float32\n",
      "  459                 model.23.dfl.conv.weight     False           16        [1, 16, 1, 1]        7.5       4.76 torch.float32\n",
      "  460     model.23.one2one_cv2.0.0.conv.weight      True       147456      [64, 256, 3, 3]  -3.91e-05      0.012 torch.float32\n",
      "  461       model.23.one2one_cv2.0.0.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  462         model.23.one2one_cv2.0.0.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  463     model.23.one2one_cv2.0.1.conv.weight      True        36864       [64, 64, 3, 3]   2.51e-05      0.024 torch.float32\n",
      "  464       model.23.one2one_cv2.0.1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  465         model.23.one2one_cv2.0.1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  466          model.23.one2one_cv2.0.2.weight      True         4096       [64, 64, 1, 1]   7.49e-05     0.0727 torch.float32\n",
      "  467            model.23.one2one_cv2.0.2.bias      True           64                 [64]          1          0 torch.float32\n",
      "  468     model.23.one2one_cv2.1.0.conv.weight      True       294912      [64, 512, 3, 3]  -3.35e-06    0.00851 torch.float32\n",
      "  469       model.23.one2one_cv2.1.0.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  470         model.23.one2one_cv2.1.0.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  471     model.23.one2one_cv2.1.1.conv.weight      True        36864       [64, 64, 3, 3]   0.000114     0.0241 torch.float32\n",
      "  472       model.23.one2one_cv2.1.1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  473         model.23.one2one_cv2.1.1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  474          model.23.one2one_cv2.1.2.weight      True         4096       [64, 64, 1, 1]   -0.00119     0.0725 torch.float32\n",
      "  475            model.23.one2one_cv2.1.2.bias      True           64                 [64]          1          0 torch.float32\n",
      "  476     model.23.one2one_cv2.2.0.conv.weight      True       294912      [64, 512, 3, 3]   2.21e-05    0.00851 torch.float32\n",
      "  477       model.23.one2one_cv2.2.0.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  478         model.23.one2one_cv2.2.0.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  479     model.23.one2one_cv2.2.1.conv.weight      True        36864       [64, 64, 3, 3]   6.06e-05      0.024 torch.float32\n",
      "  480       model.23.one2one_cv2.2.1.bn.weight      True           64                 [64]          1          0 torch.float32\n",
      "  481         model.23.one2one_cv2.2.1.bn.bias      True           64                 [64]          0          0 torch.float32\n",
      "  482          model.23.one2one_cv2.2.2.weight      True         4096       [64, 64, 1, 1]   0.000523      0.073 torch.float32\n",
      "  483            model.23.one2one_cv2.2.2.bias      True           64                 [64]          1          0 torch.float32\n",
      "  484   model.23.one2one_cv3.0.0.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00337      0.194 torch.float32\n",
      "  485     model.23.one2one_cv3.0.0.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  486       model.23.one2one_cv3.0.0.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  487   model.23.one2one_cv3.0.0.1.conv.weight      True        65536     [256, 256, 1, 1]  -8.95e-05     0.0361 torch.float32\n",
      "  488     model.23.one2one_cv3.0.0.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  489       model.23.one2one_cv3.0.0.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  490   model.23.one2one_cv3.0.1.0.conv.weight      True         2304       [256, 1, 3, 3]  -0.000615       0.19 torch.float32\n",
      "  491     model.23.one2one_cv3.0.1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  492       model.23.one2one_cv3.0.1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  493   model.23.one2one_cv3.0.1.1.conv.weight      True        65536     [256, 256, 1, 1]   7.58e-05     0.0361 torch.float32\n",
      "  494     model.23.one2one_cv3.0.1.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  495       model.23.one2one_cv3.0.1.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  496          model.23.one2one_cv3.0.2.weight      True        20480      [80, 256, 1, 1]    -0.0003      0.036 torch.float32\n",
      "  497            model.23.one2one_cv3.0.2.bias      True           80                 [80]      -11.5   1.92e-06 torch.float32\n",
      "  498   model.23.one2one_cv3.1.0.0.conv.weight      True         4608       [512, 1, 3, 3]    0.00116      0.191 torch.float32\n",
      "  499     model.23.one2one_cv3.1.0.0.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  500       model.23.one2one_cv3.1.0.0.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  501   model.23.one2one_cv3.1.0.1.conv.weight      True       131072     [256, 512, 1, 1]   0.000104     0.0256 torch.float32\n",
      "  502     model.23.one2one_cv3.1.0.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  503       model.23.one2one_cv3.1.0.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  504   model.23.one2one_cv3.1.1.0.conv.weight      True         2304       [256, 1, 3, 3]    0.00638      0.192 torch.float32\n",
      "  505     model.23.one2one_cv3.1.1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  506       model.23.one2one_cv3.1.1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  507   model.23.one2one_cv3.1.1.1.conv.weight      True        65536     [256, 256, 1, 1]   8.33e-06      0.036 torch.float32\n",
      "  508     model.23.one2one_cv3.1.1.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  509       model.23.one2one_cv3.1.1.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  510          model.23.one2one_cv3.1.2.weight      True        20480      [80, 256, 1, 1]    0.00018      0.036 torch.float32\n",
      "  511            model.23.one2one_cv3.1.2.bias      True           80                 [80]      -10.2          0 torch.float32\n",
      "  512   model.23.one2one_cv3.2.0.0.conv.weight      True         4608       [512, 1, 3, 3]   -0.00115      0.193 torch.float32\n",
      "  513     model.23.one2one_cv3.2.0.0.bn.weight      True          512                [512]          1          0 torch.float32\n",
      "  514       model.23.one2one_cv3.2.0.0.bn.bias      True          512                [512]          0          0 torch.float32\n",
      "  515   model.23.one2one_cv3.2.0.1.conv.weight      True       131072     [256, 512, 1, 1]   0.000156     0.0256 torch.float32\n",
      "  516     model.23.one2one_cv3.2.0.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  517       model.23.one2one_cv3.2.0.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  518   model.23.one2one_cv3.2.1.0.conv.weight      True         2304       [256, 1, 3, 3]   -0.00224      0.192 torch.float32\n",
      "  519     model.23.one2one_cv3.2.1.0.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  520       model.23.one2one_cv3.2.1.0.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  521   model.23.one2one_cv3.2.1.1.conv.weight      True        65536     [256, 256, 1, 1]   8.46e-05      0.036 torch.float32\n",
      "  522     model.23.one2one_cv3.2.1.1.bn.weight      True          256                [256]          1          0 torch.float32\n",
      "  523       model.23.one2one_cv3.2.1.1.bn.bias      True          256                [256]          0          0 torch.float32\n",
      "  524          model.23.one2one_cv3.2.2.weight      True        20480      [80, 256, 1, 1]   0.000216     0.0361 torch.float32\n",
      "  525            model.23.one2one_cv3.2.2.bias      True           80                 [80]      -8.76          0 torch.float32\n",
      "YOLOv10l summary: 628 layers, 25888688 parameters, 25888672 gradients, 127.9 GFLOPs\n",
      "(628, 25888688, 25888672, 127.87471359999999)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# 加载模型\n",
    "model = YOLO(task='detect', model = r\"E:\\PycharmProjects\\ultralytics\\ultralytics\\cfg\\models\\v10\\yolov10l.yaml\")  # 从头开始构建新模型\n",
    "print(model.info(detailed=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bf06d-94ed-437a-b27a-2264fc219abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}